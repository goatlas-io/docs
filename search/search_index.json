{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atlas \u00b6 Welcome to Atlas, the project that makes running Thanos at scale with securely not only possible, but simple. Note First and foremost thank you to Bartek Plotka who initially shared with me the Envoy strategy a couple years ago that ultimately led to the creation of this project. Important The documentation is a work in progress, I've attempted to keep it as accurate as possible but there are some additional features in the works that may or may not be referenced in the docs. Apologies for any confusion. Overview \u00b6 Atlas provides the ability to easily run a secure distributed Thanos deployment. Atlas at it's core is a small set of kubernetes operators that uses services and secrets resources as the underlying source of truth to populate a customized Envoy Aggreggated Service Discovery server which the Envoy proxies connect to and obtain their configurations to create the secure distributed envoy network that Thanos then traverses for connectivity. Atlas provides Thanos Query the ability to connect to Thanos Sidecars securely over HTTP/2 authenticated via Mutual TLS. Additionally when an ingress on the observability cluster (where Atlas is installed) is configured properly, you can access every downstream cluster's individual Prometheus and Alert Manager web interfaces. Finally Atlas provides the ability for EVERY downstream cluster's Prometheus instances to securely send alerts back to the observability alert managers over the HTTP/2 protected by Mutual TLS. This means that you can protect access to the alertmanager with something like an oauth2 proxy and not worry about how to allow the Prometheus instances to authenticate to it for sending alerts. Atlas does not deploy Thanos or configure Thanos for you. Please see Atlas documentation on how to configure Thanos to use Atlas. Features \u00b6 End to End Encryption using HTTP/2 and Mutual TLS Automatic Service Discovery for Downstream Cluster Thanos Sidecars Access Downstream Prometheus Instances through Envoy Proxy Network Access Downstream Alert Manager Instances through Envoy Proxy Network Access Downstream Thanos Instances through Envoy Proxy Network Allow Downstream Prometheus to send alerts to Observability Alert Manager Instances Rotate the TLS material uses by the Envoy Proxies PKI \u00b6 For the initial release, Atlas manages it's own CA certificate and signing information. It is generated unique per install, this is to reduce friction in getting it deployed. Future versions may support cert-manager. Requirements \u00b6 Must be able to modify CoreDNS config map for the cluster OR override DNS servers for thanos query components. Must be able to deploy an Envoy Proxy to each downstream cluster (Atlas provides the Helm Values for the Envoy Chart). Must be able to expose ports 10900-10904 on the observability cluster Envoy Proxy (Envoy must handle TLS termination). Must be able to expose ports 11901-11904 on the downstream cluster Envy Proxy (Envoy must handle TLS termination). Installation \u00b6 The installation is broken down between the Observability cluster which is where Atlas is installed and then downstream clusters which are what Atlas ends up providing access to. All deployments are done via Helm. The Atlas command line binary provides commands to add a cluster and retrieve it's helm values. Once you've retrieved the helm values you can install the envoy proxy on the downstream clusters. See the full deployment documentation","title":"Home"},{"location":"#atlas","text":"Welcome to Atlas, the project that makes running Thanos at scale with securely not only possible, but simple. Note First and foremost thank you to Bartek Plotka who initially shared with me the Envoy strategy a couple years ago that ultimately led to the creation of this project. Important The documentation is a work in progress, I've attempted to keep it as accurate as possible but there are some additional features in the works that may or may not be referenced in the docs. Apologies for any confusion.","title":"Atlas"},{"location":"#overview","text":"Atlas provides the ability to easily run a secure distributed Thanos deployment. Atlas at it's core is a small set of kubernetes operators that uses services and secrets resources as the underlying source of truth to populate a customized Envoy Aggreggated Service Discovery server which the Envoy proxies connect to and obtain their configurations to create the secure distributed envoy network that Thanos then traverses for connectivity. Atlas provides Thanos Query the ability to connect to Thanos Sidecars securely over HTTP/2 authenticated via Mutual TLS. Additionally when an ingress on the observability cluster (where Atlas is installed) is configured properly, you can access every downstream cluster's individual Prometheus and Alert Manager web interfaces. Finally Atlas provides the ability for EVERY downstream cluster's Prometheus instances to securely send alerts back to the observability alert managers over the HTTP/2 protected by Mutual TLS. This means that you can protect access to the alertmanager with something like an oauth2 proxy and not worry about how to allow the Prometheus instances to authenticate to it for sending alerts. Atlas does not deploy Thanos or configure Thanos for you. Please see Atlas documentation on how to configure Thanos to use Atlas.","title":"Overview"},{"location":"#features","text":"End to End Encryption using HTTP/2 and Mutual TLS Automatic Service Discovery for Downstream Cluster Thanos Sidecars Access Downstream Prometheus Instances through Envoy Proxy Network Access Downstream Alert Manager Instances through Envoy Proxy Network Access Downstream Thanos Instances through Envoy Proxy Network Allow Downstream Prometheus to send alerts to Observability Alert Manager Instances Rotate the TLS material uses by the Envoy Proxies","title":"Features"},{"location":"#pki","text":"For the initial release, Atlas manages it's own CA certificate and signing information. It is generated unique per install, this is to reduce friction in getting it deployed. Future versions may support cert-manager.","title":"PKI"},{"location":"#requirements","text":"Must be able to modify CoreDNS config map for the cluster OR override DNS servers for thanos query components. Must be able to deploy an Envoy Proxy to each downstream cluster (Atlas provides the Helm Values for the Envoy Chart). Must be able to expose ports 10900-10904 on the observability cluster Envoy Proxy (Envoy must handle TLS termination). Must be able to expose ports 11901-11904 on the downstream cluster Envy Proxy (Envoy must handle TLS termination).","title":"Requirements"},{"location":"#installation","text":"The installation is broken down between the Observability cluster which is where Atlas is installed and then downstream clusters which are what Atlas ends up providing access to. All deployments are done via Helm. The Atlas command line binary provides commands to add a cluster and retrieve it's helm values. Once you've retrieved the helm values you can install the envoy proxy on the downstream clusters. See the full deployment documentation","title":"Installation"},{"location":"components/","text":"Components \u00b6 Atlas is made up of 4 components, but only 2 are part of it's source code, the other two are external. Controller Envoy ADS Server CoreDNS Envoy Proxy Controller \u00b6 The controller (aka operator) is what ensures all the services, secrets and configmaps are in the correct state. The controller is responsible for taking the service definitions that represent a downstream cluster and ensure that all subsequent resources are created and managed properly. Atlas creates services for each Thanos replica that exists in the downstream cluster and also generates Helm Values for the Envoy helm chart deployment for the downstream clusters. There are a number of annotations supported by Atlas that allow for the configuration and tweaking of a downstream cluster should it be necessary, the controller also handles those to ensure that everything is configured properly. Envoy ADS Server \u00b6 The Envoy ADS Server watches the kubernetes cluster for changes to services looking for those with the right annotations to mark them as an Atlas Cluster. It also finds and identifies all the PKI related secrets that have been stored by Atlas as well. The Envoy ADS Server then takes all that data and generates all the various Envoy Proxy configurations and creates snapshots for the Envoy Proxies to obtain. As services and secrets change, the Envoy ADS server automatically re-generates configurations as needed and announces the changes so that the connected Envoy Proxy instances will pick up their new configurations. CoreDNS \u00b6 Atlas creates and keeps up-to-date a DNS zone file based on the service information within the observability cluster, the CoreDNS server deployed by the Atlas Helm Chart is set to read in the zone file and reload it when the file changes. The Thanos Querier component is the consumer of the DNS zone records. Be defining a DNS service discovery query for Thanos Query to use, it will discover all the downstream cluster Thanos Sidecars and load them. There are two ways to configure Thanos Query to use the DNS either my modifying the cluster's CoreDNS configuration to point the Atlas TLD to the CoreDNS server deployed by Atlas OR point the Thanos Querier deployment DNS server to the CoreDNS instance deployed by Atlas. Envoy Proxy \u00b6 The Envoy Proxy is used both on the observability cluster and the downstream cluster to provide secure communications between all the Thanos and Prometheus components. By leveraging it's service discovery capabilities, Atlas can generate and maintain all the appropriate configurations and the Envoy Proxy instances simply reconfigure themselves as needed. The downstream Envoy Proxy instance configurations stay fairly static, since very little changes there, however depending on how often you are adding or removing downstream clusters the observability Envoy Proxy instance's configuration changes quite frequently. However thanks to the Envoy's internal mechanisms these changes happen in real-time and do not require a restart or reload.","title":"Components"},{"location":"components/#components","text":"Atlas is made up of 4 components, but only 2 are part of it's source code, the other two are external. Controller Envoy ADS Server CoreDNS Envoy Proxy","title":"Components"},{"location":"components/#controller","text":"The controller (aka operator) is what ensures all the services, secrets and configmaps are in the correct state. The controller is responsible for taking the service definitions that represent a downstream cluster and ensure that all subsequent resources are created and managed properly. Atlas creates services for each Thanos replica that exists in the downstream cluster and also generates Helm Values for the Envoy helm chart deployment for the downstream clusters. There are a number of annotations supported by Atlas that allow for the configuration and tweaking of a downstream cluster should it be necessary, the controller also handles those to ensure that everything is configured properly.","title":"Controller"},{"location":"components/#envoy-ads-server","text":"The Envoy ADS Server watches the kubernetes cluster for changes to services looking for those with the right annotations to mark them as an Atlas Cluster. It also finds and identifies all the PKI related secrets that have been stored by Atlas as well. The Envoy ADS Server then takes all that data and generates all the various Envoy Proxy configurations and creates snapshots for the Envoy Proxies to obtain. As services and secrets change, the Envoy ADS server automatically re-generates configurations as needed and announces the changes so that the connected Envoy Proxy instances will pick up their new configurations.","title":"Envoy ADS Server"},{"location":"components/#coredns","text":"Atlas creates and keeps up-to-date a DNS zone file based on the service information within the observability cluster, the CoreDNS server deployed by the Atlas Helm Chart is set to read in the zone file and reload it when the file changes. The Thanos Querier component is the consumer of the DNS zone records. Be defining a DNS service discovery query for Thanos Query to use, it will discover all the downstream cluster Thanos Sidecars and load them. There are two ways to configure Thanos Query to use the DNS either my modifying the cluster's CoreDNS configuration to point the Atlas TLD to the CoreDNS server deployed by Atlas OR point the Thanos Querier deployment DNS server to the CoreDNS instance deployed by Atlas.","title":"CoreDNS"},{"location":"components/#envoy-proxy","text":"The Envoy Proxy is used both on the observability cluster and the downstream cluster to provide secure communications between all the Thanos and Prometheus components. By leveraging it's service discovery capabilities, Atlas can generate and maintain all the appropriate configurations and the Envoy Proxy instances simply reconfigure themselves as needed. The downstream Envoy Proxy instance configurations stay fairly static, since very little changes there, however depending on how often you are adding or removing downstream clusters the observability Envoy Proxy instance's configuration changes quite frequently. However thanks to the Envoy's internal mechanisms these changes happen in real-time and do not require a restart or reload.","title":"Envoy Proxy"},{"location":"configuration/","text":"Configuration \u00b6 Labels \u00b6 Atlas leverages a few labels to identify which services it should care about, all others it ignores. goatlas.io/cluster - This is used on a service to identify it as a record with external IPs as a downstream cluster goatlas.io/replicas - The number of Thanos replicas in the downstream clusters Annotations \u00b6 Atlas currently relies upon existing resource definitions within Kubernetes to work and therefore relies on annotations on services for configuration. goatlas.io/envoy-selectors \u00b6 Default: app=envoy,release=atlas Resource: service This annotation is used to override the selector labels uses to identify the Envoy Proxy service that all traffic should be routed through on the downstream cluster, this is important when you deviate from the default deployment naming conventions. goatlas.io/thanos-service \u00b6 Default: prometheus-operated.monitoring.svc.cluster.local Resource: service This annotation is used to change the default fully qualified domain name on the downstream cluster where the thanos sidecar can be reached. goatlas.io/prometheus-service \u00b6 Default: prometheus-operated.monitoring.svc.cluster.local Resource: service This annotation is used to change the default fully qualified domain name on the downstream cluster where the prometheus instance can be reached. goatlas.io/alertmanager-service \u00b6 Default: alertmanager-operated.monitoring.svc.cluster.local Resource: service This annotation is used to change the default fully qualified domain name on the downstream cluster where the alertmanager instance can be reached. Ingress Setup for Prometheus Access \u00b6 The helm chart takes care of all ingresses for Atlas, however there are additional ingress tweaks you may elect to perform should you want to use the full power of Atlas. Atlas can provide access to all downstream prometheus instances and alert manager instances using the Envoy Proxy network it establishes, but you have to configure an ingress with path prefixing to route the traffic. In the real world I use the ingress that I use to expose thanos-query to also expose access to the downstream Prometheus instances and the downstream Alert Managers. On your ingress modify it to have two path prefixes you can leverage the envoy network that allows thanos to communicate securely to access the individual prometheus, thanos sidecar and alert manager instances that exist in the downstream cluster using the observability envoy proxy. Essentially what this looks like is that the ingress that manages the flow for the thanos-query, you can add a path prefix for /prom and point it to the envoy proxy that was deployed on the observability cluster, the result then allows you to hit /prom/downstream-cluster-name/ in a browser and have direct access to the prometheus instance. This is especially helpful for debugging. http : paths : - backend : service : name : thanos-query port : number : 9090 path : / pathType : Prefix - backend : service : name : envoy port : number : 10904 path : /prom pathType : Prefix","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#labels","text":"Atlas leverages a few labels to identify which services it should care about, all others it ignores. goatlas.io/cluster - This is used on a service to identify it as a record with external IPs as a downstream cluster goatlas.io/replicas - The number of Thanos replicas in the downstream clusters","title":"Labels"},{"location":"configuration/#annotations","text":"Atlas currently relies upon existing resource definitions within Kubernetes to work and therefore relies on annotations on services for configuration.","title":"Annotations"},{"location":"configuration/#goatlasioenvoy-selectors","text":"Default: app=envoy,release=atlas Resource: service This annotation is used to override the selector labels uses to identify the Envoy Proxy service that all traffic should be routed through on the downstream cluster, this is important when you deviate from the default deployment naming conventions.","title":"goatlas.io/envoy-selectors"},{"location":"configuration/#goatlasiothanos-service","text":"Default: prometheus-operated.monitoring.svc.cluster.local Resource: service This annotation is used to change the default fully qualified domain name on the downstream cluster where the thanos sidecar can be reached.","title":"goatlas.io/thanos-service"},{"location":"configuration/#goatlasioprometheus-service","text":"Default: prometheus-operated.monitoring.svc.cluster.local Resource: service This annotation is used to change the default fully qualified domain name on the downstream cluster where the prometheus instance can be reached.","title":"goatlas.io/prometheus-service"},{"location":"configuration/#goatlasioalertmanager-service","text":"Default: alertmanager-operated.monitoring.svc.cluster.local Resource: service This annotation is used to change the default fully qualified domain name on the downstream cluster where the alertmanager instance can be reached.","title":"goatlas.io/alertmanager-service"},{"location":"configuration/#ingress-setup-for-prometheus-access","text":"The helm chart takes care of all ingresses for Atlas, however there are additional ingress tweaks you may elect to perform should you want to use the full power of Atlas. Atlas can provide access to all downstream prometheus instances and alert manager instances using the Envoy Proxy network it establishes, but you have to configure an ingress with path prefixing to route the traffic. In the real world I use the ingress that I use to expose thanos-query to also expose access to the downstream Prometheus instances and the downstream Alert Managers. On your ingress modify it to have two path prefixes you can leverage the envoy network that allows thanos to communicate securely to access the individual prometheus, thanos sidecar and alert manager instances that exist in the downstream cluster using the observability envoy proxy. Essentially what this looks like is that the ingress that manages the flow for the thanos-query, you can add a path prefix for /prom and point it to the envoy proxy that was deployed on the observability cluster, the result then allows you to hit /prom/downstream-cluster-name/ in a browser and have direct access to the prometheus instance. This is especially helpful for debugging. http : paths : - backend : service : name : thanos-query port : number : 9090 path : / pathType : Prefix - backend : service : name : envoy port : number : 10904 path : /prom pathType : Prefix","title":"Ingress Setup for Prometheus Access"},{"location":"deployment/","text":"Deployment \u00b6 You should have at least two clusters to take full advantage of Atlas. One to act as the observability cluster and the other as a downstream cluster, if you have more than two clusters, all the others are downstream clusters too. Atlas should only be installed to the observability cluster. All downstream clusters will need an envoy instance deployed, Atlas will provide the necessary helm values to configure the downstream clusters. Important It is HIGHLY recommend using the same namespace for your observability components, it makes deployment management much easier. The default for atlas is monitoring . Requirements \u00b6 1 Cluster to act as the Observability Cluster 1 Cluster to act as a Downstream Cluster Ability to install helm charts The envoy helm chart must be installed to an edge node (typically where an ingress instance would be deployed) Deploy Prometheus with Thanos Sidecar \u00b6 It is recommended you use the same namespace like monitoring for the deployment of Prometheus and Atlas. How you deploy Prometheus with the Thanos Sidecar is up to you, however I would recommend simply using the kube-prometheus-stack helm chart as it makes this process very simple and takes care of the more complicated bits for you. If you want Thanos persisting to S3 you can pass your S3 credentials along as well. Note When using kube-prometheus-stack ensure servicePerReplica is enabled for both prometheus and alertmanager sections, this will allow proper routing to each individual instance. Once you have your Prometheus instances deployed, please make sure to note the service name as it will be necessary for configuring Atlas properly. If you are use kube-prometheus-stack most of the defaults will work out of the box. If you are using something non-standard, please make sure that the Prometheus Port and Thanos Sidecar ports are on the service. Step 1. Deploying Atlas \u00b6 The first step is to deploy Atlas to your observability cluster. helm install atlas chart/ Step 2. Modify CoreDNS Configuration \u00b6 Note I highly recommend using GitOps for modifying and configuring the configmap kube-system/coredns Using your favorite method, you will need to edit the coredns config map in the kube-system namespace. Add the following to the Corefile section. atlas:53 { errors cache 30 forward . 10.43.43.10 } The complete version should look something like the following. apiVersion : v1 data : Corefile : | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } hosts /etc/coredns/NodeHosts { ttl 60 reload 15s fallthrough } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } atlas:53 { errors cache 30 forward . 10.43.43.10 } NodeHosts : | 172.20.0.2 k3d-atlas-server-0 kind : ConfigMap Step 3. Add Downstream Cluster to Observability Cluster \u00b6 Telling Atlas about a downstream cluster is as simple as adding a Service resource to your observability cluster or you can use the Atlas binary. Using YAML \u00b6 Be sure to change the name , namespace and the externalIPs section to the appropriate values. apiVersion : v1 kind : Service metadata : labels : goatlas.io/cluster : \"true\" goatlas.io/replicas : \"1\" name : downstream1 namespace : monitoring spec : clusterIP : None clusterIPs : - None externalIPs : - 1.1.1.1 ports : - name : prometheus port : 9090 protocol : TCP targetPort : 9090 - name : grpc port : 10901 protocol : TCP targetPort : 10901 - name : http port : 10902 protocol : TCP targetPort : 10902 Using the CLI \u00b6 atlas cluster-add --name \"downstream1\" --replicas 1 --external-ip \"1.1.1.1\" Step 4. Deploy Envoy on Downstream Cluster \u00b6 Atlas generates helm values for the Atlas Envoy Helm Chart for every downstream cluster added. These values come with the necessary seed values to allow initial secure connections to be established. Once comms are established the Envoy Aggreggated Discovery capabilites take over ensuring the downstream envoy instance stays configure properly. Retrieve the downstream's helm values with the atlas or kubectl atlas cluster-values --name \"downstream1\" > downstream1.yaml Note: This command has --format option, the default is raw which is just values for helm. The other options are helm-chart and helm-release helm-chart -- this is a feature from Rancher on K3S clusters helm-release -- this is for Flux V2 raw -- just values for helm install/upgrade commands OR kubectl get secret -n monitoring downstream1-envoy-values -o json | jq -r ' .data. \"values.yaml\" | base64 -D > downstream1.yaml Once you have the values, install helm on your downstream cluster. Make sure you switch to your downstream cluster context now. helm install envoy --values downstream1.yaml chart/ Step 5. Repeat \u00b6 If you have more than one downstream cluster, repeast steps 3 and 4 until you've added all your clusters. Step 6. Configure Downstream Prometheus for Observability Alertmanagers \u00b6 To take full advantage of what Atlas offers, you can configure your downstream prometheus instances to talk to the alertmanagers in the observability cluster. You'll need to add an alertmanager entry per the number of alertmanagr instances that are on the observability cluster to the downstream prometheus instance. If you are using the prometheus operator then you can simple add an additional alert managers configuration like the following. apiVersion : v1 kind : Secret metadata : name : additional-alertmanager-configs namespace : monitoring data : config.yaml : | - scheme: http static_configs: - targets: - %s","title":"Deployment"},{"location":"deployment/#deployment","text":"You should have at least two clusters to take full advantage of Atlas. One to act as the observability cluster and the other as a downstream cluster, if you have more than two clusters, all the others are downstream clusters too. Atlas should only be installed to the observability cluster. All downstream clusters will need an envoy instance deployed, Atlas will provide the necessary helm values to configure the downstream clusters. Important It is HIGHLY recommend using the same namespace for your observability components, it makes deployment management much easier. The default for atlas is monitoring .","title":"Deployment"},{"location":"deployment/#requirements","text":"1 Cluster to act as the Observability Cluster 1 Cluster to act as a Downstream Cluster Ability to install helm charts The envoy helm chart must be installed to an edge node (typically where an ingress instance would be deployed)","title":"Requirements"},{"location":"deployment/#deploy-prometheus-with-thanos-sidecar","text":"It is recommended you use the same namespace like monitoring for the deployment of Prometheus and Atlas. How you deploy Prometheus with the Thanos Sidecar is up to you, however I would recommend simply using the kube-prometheus-stack helm chart as it makes this process very simple and takes care of the more complicated bits for you. If you want Thanos persisting to S3 you can pass your S3 credentials along as well. Note When using kube-prometheus-stack ensure servicePerReplica is enabled for both prometheus and alertmanager sections, this will allow proper routing to each individual instance. Once you have your Prometheus instances deployed, please make sure to note the service name as it will be necessary for configuring Atlas properly. If you are use kube-prometheus-stack most of the defaults will work out of the box. If you are using something non-standard, please make sure that the Prometheus Port and Thanos Sidecar ports are on the service.","title":"Deploy Prometheus with Thanos Sidecar"},{"location":"deployment/#step-1-deploying-atlas","text":"The first step is to deploy Atlas to your observability cluster. helm install atlas chart/","title":"Step 1. Deploying Atlas"},{"location":"deployment/#step-2-modify-coredns-configuration","text":"Note I highly recommend using GitOps for modifying and configuring the configmap kube-system/coredns Using your favorite method, you will need to edit the coredns config map in the kube-system namespace. Add the following to the Corefile section. atlas:53 { errors cache 30 forward . 10.43.43.10 } The complete version should look something like the following. apiVersion : v1 data : Corefile : | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } hosts /etc/coredns/NodeHosts { ttl 60 reload 15s fallthrough } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } atlas:53 { errors cache 30 forward . 10.43.43.10 } NodeHosts : | 172.20.0.2 k3d-atlas-server-0 kind : ConfigMap","title":"Step 2. Modify CoreDNS Configuration"},{"location":"deployment/#step-3-add-downstream-cluster-to-observability-cluster","text":"Telling Atlas about a downstream cluster is as simple as adding a Service resource to your observability cluster or you can use the Atlas binary.","title":"Step 3. Add Downstream Cluster to Observability Cluster"},{"location":"deployment/#using-yaml","text":"Be sure to change the name , namespace and the externalIPs section to the appropriate values. apiVersion : v1 kind : Service metadata : labels : goatlas.io/cluster : \"true\" goatlas.io/replicas : \"1\" name : downstream1 namespace : monitoring spec : clusterIP : None clusterIPs : - None externalIPs : - 1.1.1.1 ports : - name : prometheus port : 9090 protocol : TCP targetPort : 9090 - name : grpc port : 10901 protocol : TCP targetPort : 10901 - name : http port : 10902 protocol : TCP targetPort : 10902","title":"Using YAML"},{"location":"deployment/#using-the-cli","text":"atlas cluster-add --name \"downstream1\" --replicas 1 --external-ip \"1.1.1.1\"","title":"Using the CLI"},{"location":"deployment/#step-4-deploy-envoy-on-downstream-cluster","text":"Atlas generates helm values for the Atlas Envoy Helm Chart for every downstream cluster added. These values come with the necessary seed values to allow initial secure connections to be established. Once comms are established the Envoy Aggreggated Discovery capabilites take over ensuring the downstream envoy instance stays configure properly. Retrieve the downstream's helm values with the atlas or kubectl atlas cluster-values --name \"downstream1\" > downstream1.yaml Note: This command has --format option, the default is raw which is just values for helm. The other options are helm-chart and helm-release helm-chart -- this is a feature from Rancher on K3S clusters helm-release -- this is for Flux V2 raw -- just values for helm install/upgrade commands OR kubectl get secret -n monitoring downstream1-envoy-values -o json | jq -r ' .data. \"values.yaml\" | base64 -D > downstream1.yaml Once you have the values, install helm on your downstream cluster. Make sure you switch to your downstream cluster context now. helm install envoy --values downstream1.yaml chart/","title":"Step 4. Deploy Envoy on Downstream Cluster"},{"location":"deployment/#step-5-repeat","text":"If you have more than one downstream cluster, repeast steps 3 and 4 until you've added all your clusters.","title":"Step 5. Repeat"},{"location":"deployment/#step-6-configure-downstream-prometheus-for-observability-alertmanagers","text":"To take full advantage of what Atlas offers, you can configure your downstream prometheus instances to talk to the alertmanagers in the observability cluster. You'll need to add an alertmanager entry per the number of alertmanagr instances that are on the observability cluster to the downstream prometheus instance. If you are using the prometheus operator then you can simple add an additional alert managers configuration like the following. apiVersion : v1 kind : Secret metadata : name : additional-alertmanager-configs namespace : monitoring data : config.yaml : | - scheme: http static_configs: - targets: - %s","title":"Step 6. Configure Downstream Prometheus for Observability Alertmanagers"},{"location":"development/","text":"Development \u00b6 Developing on Atlas is fairly simple but running all the various components to verify connectivity can be tough. It is recommended that k3d be used which leverages k3s . Steps \u00b6 Create k3s Cluster k3d cluster create atlas --api-port 192.168.11.103:6556 (subsitute the IP for the docker host IP) Obtain k3s kubeconfig k3d kubeconfig get atlas > atlas-kubeconfig.yaml Save the contents from step 2 to a file called altas-kubeconfig.yaml Setup your terminal to use the file export KUBECONFIG=./atlas-kubeconfig.yaml Now you are setup to do development, when you run atlas, it'll use the correct kubeconfig, alternatively you can specify it on the command line.","title":"Development"},{"location":"development/#development","text":"Developing on Atlas is fairly simple but running all the various components to verify connectivity can be tough. It is recommended that k3d be used which leverages k3s .","title":"Development"},{"location":"development/#steps","text":"Create k3s Cluster k3d cluster create atlas --api-port 192.168.11.103:6556 (subsitute the IP for the docker host IP) Obtain k3s kubeconfig k3d kubeconfig get atlas > atlas-kubeconfig.yaml Save the contents from step 2 to a file called altas-kubeconfig.yaml Setup your terminal to use the file export KUBECONFIG=./atlas-kubeconfig.yaml Now you are setup to do development, when you run atlas, it'll use the correct kubeconfig, alternatively you can specify it on the command line.","title":"Steps"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Why not use a Custom Resource Definition (aka CRD)? \u00b6 The current built in resources work well and at the initial offering of Atlas, it didn't make sense to go the route of a CRD, however it's easy enough to switch to or add in later.","title":"F.A.Q."},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#why-not-use-a-custom-resource-definition-aka-crd","text":"The current built in resources work well and at the initial offering of Atlas, it didn't make sense to go the route of a CRD, however it's easy enough to switch to or add in later.","title":"Why not use a Custom Resource Definition (aka CRD)?"},{"location":"quick-start/","text":"Quick Start (aka Demo using Digital Ocean) \u00b6 Important This script requires a Digital Ocean account. The easiest way to get started is to take the deploy script for a spin. This script spins up a fully working Atlas deployment with Prometheus and Thanos on several downstream clusters and automatically configures the downstream clusters for secure communication. Preflight \u00b6 Obtain a Digital Ocean Access Token Obtain the SSH Key ID of your SSH key in Digital Ocean export DIGITALOCEAN_ACCESS_TOKEN=<access-token> export DIGITALOCEAN_SSH_KEYS=<key-id> Note I'd highly recommend the use of direnv for managing environment variables throughout directories. Deploy \u00b6 bash examples/demo-do/deploy.sh up Note This script takes approximately 5-7 minutes to run, depending on how fast Digital Ocean is. It's spinning up a total of 4 servers and installing k3s , then using helm to install the necessary components like prometheus, thanos, envoy and atlas on the various servers. This script will deploy four clusters: observability downstream1 downstream2 downstream3 Once the script is done running a set of details will be printed to the screen. If you want to see the details again simply re-run the script with details instead of up . The details output will give you all the urls to the various components that can be interacted with on the observability cluster and the downstream clusters, see below for more details. Generally speaking by the time the details page shows up downstream1 and downstream2 will already be connected. Downstream3 will still be in the process of coming online, but should only take another minute or two at most. Details \u00b6 In general your details will look something like the following ... IP Addresses ----------------------------------------- Observability: 143.198.182.161 Downstream1: 198.211.117.92 Downstream2: 143.244.174.92 Downstream3: 137.184.97.135 Observability Cluster ------------------------------------------ thanos-query: http://thanos-query.143.198.182.161.nip.io prometheus: http://prometheus.143.198.182.161.nip.io alerts: http://alerts.143.198.182.161.nip.io Accessing Downstream through Observability Cluster: Note: these use the Envoy Proxy Network provided by Atlas to allow secure communications to downstream cluster components. downstream1: http://thanos-query.143.198.182.161.nip.io/prom/downstream1/graph downstream2: http://thanos-query.143.198.182.161.nip.io/prom/downstream2/graph downstream3: http://thanos-query.143.198.182.161.nip.io/prom/downstream3/graph Important: In a real-world scenario you'd gate access to thanos-query via an oauth2 proxy or it would only be accessible on an internal network! The link to thanos-query in the observability cluster is how you can see your thanos query connected to the sidecars. The downstream1-3 links all use the thanos-query and the ingress path prefix that allows accessing of the downstream clusters from the observability cluster. You can confirm this by going to each link and pulling up the prometheus configuration, you'll see the external labels differ for each one. Teardown \u00b6 When you are all done, bash examples/demo-do/depoy.sh down to tear it all down.","title":"Quick Start"},{"location":"quick-start/#quick-start-aka-demo-using-digital-ocean","text":"Important This script requires a Digital Ocean account. The easiest way to get started is to take the deploy script for a spin. This script spins up a fully working Atlas deployment with Prometheus and Thanos on several downstream clusters and automatically configures the downstream clusters for secure communication.","title":"Quick Start (aka Demo using Digital Ocean)"},{"location":"quick-start/#preflight","text":"Obtain a Digital Ocean Access Token Obtain the SSH Key ID of your SSH key in Digital Ocean export DIGITALOCEAN_ACCESS_TOKEN=<access-token> export DIGITALOCEAN_SSH_KEYS=<key-id> Note I'd highly recommend the use of direnv for managing environment variables throughout directories.","title":"Preflight"},{"location":"quick-start/#deploy","text":"bash examples/demo-do/deploy.sh up Note This script takes approximately 5-7 minutes to run, depending on how fast Digital Ocean is. It's spinning up a total of 4 servers and installing k3s , then using helm to install the necessary components like prometheus, thanos, envoy and atlas on the various servers. This script will deploy four clusters: observability downstream1 downstream2 downstream3 Once the script is done running a set of details will be printed to the screen. If you want to see the details again simply re-run the script with details instead of up . The details output will give you all the urls to the various components that can be interacted with on the observability cluster and the downstream clusters, see below for more details. Generally speaking by the time the details page shows up downstream1 and downstream2 will already be connected. Downstream3 will still be in the process of coming online, but should only take another minute or two at most.","title":"Deploy"},{"location":"quick-start/#details","text":"In general your details will look something like the following ... IP Addresses ----------------------------------------- Observability: 143.198.182.161 Downstream1: 198.211.117.92 Downstream2: 143.244.174.92 Downstream3: 137.184.97.135 Observability Cluster ------------------------------------------ thanos-query: http://thanos-query.143.198.182.161.nip.io prometheus: http://prometheus.143.198.182.161.nip.io alerts: http://alerts.143.198.182.161.nip.io Accessing Downstream through Observability Cluster: Note: these use the Envoy Proxy Network provided by Atlas to allow secure communications to downstream cluster components. downstream1: http://thanos-query.143.198.182.161.nip.io/prom/downstream1/graph downstream2: http://thanos-query.143.198.182.161.nip.io/prom/downstream2/graph downstream3: http://thanos-query.143.198.182.161.nip.io/prom/downstream3/graph Important: In a real-world scenario you'd gate access to thanos-query via an oauth2 proxy or it would only be accessible on an internal network! The link to thanos-query in the observability cluster is how you can see your thanos query connected to the sidecars. The downstream1-3 links all use the thanos-query and the ingress path prefix that allows accessing of the downstream clusters from the observability cluster. You can confirm this by going to each link and pulling up the prometheus configuration, you'll see the external labels differ for each one.","title":"Details"},{"location":"quick-start/#teardown","text":"When you are all done, bash examples/demo-do/depoy.sh down to tear it all down.","title":"Teardown"}]}